<!doctype html><html lang=en-us><head><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge,chrome=1"><meta name=viewport content="width=device-width,initial-scale=1"><meta name=author content="lumostone"><title>Guide to Ethereum 2.0 Staking with Kubernetes and Prysm ｜ lumostone</title><meta name=description content="Why Stake with Kubernetes? As stakers, we all want to minimize downtime and the risk of being slashed.
Minimizing downtime is a difficult objective to achieve since a validator might be down for various reasons: system failures, incomplete restart policies, connectivity issues, hardware maintenance or software bugs. Staking with two or more machines for redundancy naturally comes to mind.
People might say, “redundancy leads to slashing!” which is a legitimate concern because we could accidentally run multiple validators with the same validator keys at the same time."><link rel="shortcut icon" href=https://lumostone.com/images/favicon.ico><link rel=stylesheet type=text/css media=screen href=https://lumostone.com/css/normalize.css><link rel=stylesheet type=text/css media=screen href=https://cdn.jsdelivr.net/npm/animate.css@4.1.0/animate.min.css><link rel=stylesheet type=text/css media=screen href=https://lumostone.com/css/zozo.css><link rel=stylesheet type=text/css media=screen href=https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css></head><body><div class="main animate__animated animate__fadeInDown"><div class="nav_container animated fadeInDown"><div class=site_nav id=site_nav><ul></ul></div><div class=menu_icon><a id=menu_icon><i class=ri-menu-line></i></a></div></div><div class="header animated fadeInDown"><div class=site_title_container><div class=site_title><h1><a href=/en><span>lumostone</span></a></h1></div><div class=description><p class=sub_title></p><div class=my_socials><a href=https://github.com/lumostone title=github target=_blank><i class=ri-github-fill></i></a></div></div></div></div><div class=content><div class=post_page><div class="post animate__animated animate__fadeInDown"><div class="post_title post_detail_title"><a href=/en/eth2-staking-with-k8s-prysm/><h2>Guide to Ethereum 2.0 Staking with Kubernetes and Prysm</h2></a></div><section class=post_meta><div class=meta><div class=info><span class=field><i class=ri-map-pin-time-line></i>
<span class=date>2021.04.12</span></span>
<span class="field tags"><i class=ri-stack-line></i>
<a href=/en/tags/ethereum/>ethereum</a>
<a href=/en/tags/kubernetes/>kubernetes</a>
<a href=/en/tags/tutorial/>tutorial</a></span></div></div></section><div class="post_content markdown"><h2 id=why-stake-with-kubernetes>Why Stake with Kubernetes?</h2><p>As stakers, we all want to minimize downtime and the risk of being slashed.</p><p>Minimizing downtime is a difficult objective to achieve since a validator might be down for various reasons: system failures, incomplete restart policies, connectivity issues, hardware maintenance or software bugs. Staking with two or more machines for redundancy naturally comes to mind.</p><p>People might say, “redundancy leads to slashing!” which is a legitimate concern because we could accidentally run multiple validators with the same validator keys at the same time. Migrating the validators from a broken machine to the other with inappropriate procedure might in turn corrupt the slashing protection database . A staker with benign intention has the risk of being slashed due to the error-prone manual operations and the complexities increase when you have a high-availability setup. Needless to say, the experience could deteriorate if a staker runs multiple validators.</p><p><em>Can we reduce the risk and the complexities of staking while running multiple validators and embracing redundancy?</em> Yes, we think <a href=https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/>Kubernetes</a> can help us manage the application’s lifecycle and automate the upgrade rollouts. The process of upgrading a client would be completed in one-step. Furthermore, migrating a client from one machine to another also would be a single command (<em>e.g.</em> kubectl drain node).</p><p>We hope this experiment and the setup guide can be a stepping stone for the community to stake with Kubernetes for Ethereum 2.0. Embrace the redundancy and stake with ease and scalability.</p><h2 id=acknowledgement>Acknowledgement</h2><p>We want to thank Ethereum Foundation for supporting this project via the <a href=https://blog.ethereum.org/2021/02/09/esp-staking-community-grantee-announcement/>Eth2 Staking Community Grants</a>. It’s an honor to contribute to this community!</p><h2 id=technologies>Technologies</h2><p>In this step-by-step guide, we run one beacon node with multiple validators in a Kuberenetes cluster for Ethereum 2.0 staking. We are using:</p><ul><li><a href=https://github.com/prysmaticlabs/prysm>Prysm</a> as the Ethereum 2.0 Client.</li><li><a href=https://microk8s.io/>MicroK8s</a> as the Kubernertes destribution (<a href=https://microk8s.io/docs>installation guide</a>).</li><li><a href=https://helm.sh/>Helm</a> to manage packages and releases.</li><li><a href=https://kubernetes.io/docs/reference/kubectl/overview/>kubectl</a> to run commands against Kubernetes clusters.</li><li>Ubuntu Server 20.04.2 LTS (x64) (<a href=https://ubuntu.com/download/server>download link</a>).</li><li><a href=https://en.wikipedia.org/wiki/Network_File_System>Network File System (NFS)</a> as beacon and validator’s persistent storage (<a href=https://ubuntu.com/server/docs/service-nfs>Guide for NFS installation and configuration on Ubuntu</a>).</li><li><a href=https://github.com/eth2xk8s/eth2xk8s>eth2xk8s</a> Helm chart.</li></ul><h2 id=goal>Goal</h2><p>This guide will help you to:</p><ul><li>Create a Kubernetes cluster with MicroK8s. If you already have your prefered Kubernetes distribution running you can jump to the section “<a href=#install-and-configure-nfs>Install and Configure NFS</a>”. If you are using managed Kubernetes services provided by cloud providers (<em>e.g.</em> AKS, EKS, and GKE), you may consider using cloud storage directly rather than NFS as the persistent storage. We will cover this topic in the future.</li><li>Install and configure NFS.</li><li>Prepare the Helm chart for multiple validators.</li><li>Install Prysm’s beacon and validator clients with the Helm chart.</li><li>Check client status.</li><li>Upgrade and roll back the Prysm’s beacon and validator clients with the Helm chart.</li></ul><h2 id=non-goal>Non-Goal</h2><p>This guide does not:</p><ul><li>Cover the performance tuning and sizing.</li><li>Cover the steps to fund the validators and to generate the validator keys.</li><li>Include Kubernetes cluster high availability (HA) configuration and security hardening.</li></ul><h2 id=disclaimer>Disclaimer</h2><p>As of today, this setup has been tested in the testnet only.</p><p>We all stake at our own risk. Please always do the experiments and dry-run on the testnet first, familiarize yourself with all the operations, and harden your systems before running it on the mainnet. This guide serves as a stepping stone for staking with Kubernetes. <strong>The authors are not responsible for any financial losses incurred by following this guide.</strong></p><h2 id=system-requirements>System Requirements</h2><p>We need at least 3 machines (virtual machines or bare-metal machines) in total for this setup. One machine will be the NFS server to store the staking data, the second machine will be the “master” node to run the Kubernetes core components, and finally, the third machine will be the “worker” node to run the workloads, which are the beacon and validators, in the Kubernetes cluster. For high availability (HA), you can consider adding more nodes by following <a href=https://microk8s.io/docs/high-availability>MicroK8s’ High Availability documentation</a> and regularly backing up the beacon data for fast startup. We will discuss HA configurations in subsequent posts.</p><p>Here are the recommended system requirements based on our testing on the <a href=https://pyrmont.beaconcha.in/><strong>Pyrmont testnet</strong></a> and MicroK8s’ <a href=https://microk8s.io/docs>official documentation</a>. Please note that meeting the minimal requirements does not guarantee optimal performance or cost efficiency.</p><p>Master:</p><ul><li>RAM: 8GB</li><li>CPU: 1 core minimum</li><li>Disk: 20 GB minimum</li></ul><p>Worker:</p><ul><li>RAM: 8GB minimum</li><li>CPU: 1 core minimum</li><li>Disk: 20 GB minimum</li></ul><p>NFS:</p><ul><li>RAM: 2GB minimum</li><li>CPU: 1 core minimum</li><li>Disk: 250 GB minimum (Again, please note it is for testnet. For running on the mainnet, you may need more storage.)</li></ul><h2 id=prerequisites>Prerequisites</h2><ul><li>You have funded your validators and have generated validator keys. If you need guidance, we recommend <a href=https://someresat.medium.com/guide-to-staking-on-ethereum-2-0-ubuntu-pyrmont-lighthouse-a634d3b87393>Somer Esat’s guide</a>.</li><li>Ethereum 1.0 “Goerli” node: <a href=https://someresat.medium.com/guide-to-staking-on-ethereum-2-0-ubuntu-pyrmont-prysm-a10b5129c7e3>Somer Esat’s guide</a> also covers steps for building the Ethereum 1.0 node. You can also choose a third-party provider such as <a href=https://infura.io/>Infura</a> or <a href=https://alchemyapi.io/>Alchemy</a>.</li><li>Planning your private network, firewall, and port forwarding. We have put our network configuration in the <a href=#overview>Walkthrough</a> for your reference.</li><li>You have installed Ubuntu Server 20.04.2 LTS (x64) on all the servers and have assigned static IPs.</li></ul><h2 id=network-requirements>Network Requirements</h2><ul><li>Every machine needs to have outbound connectivity to the Internet at least during installation.</li><li>Masters and workers can reach to each other. We will configure the firewall in the following section to only allow the inbound traffic to the ports required by MicroK8s. For more details, you can refer to <a href=https://microk8s.io/docs/ports>MicroK8s’ documentation: Services and ports</a>.</li><li>Masters and workers can reach the NFS server.</li><li>Masters and workers can reach the endpoint of the Ethereum 1.0 node.</li></ul><h2 id=walkthrough>Walkthrough</h2><h3 id=overview>Overview</h3><p>In this walkthrough, we will set up a Kubernetes cluster and a NFS server and install the beacon node and the validator clients. We put all the machines in the same private subnet and have assigned a static private IP for each machine. Here are the network configurations we use throughout this guide for the three machines:</p><p><strong>Private subnet :172.20.0.0/20 (172.20.0.1 - 172.20.15.254)</strong></p><ul><li>NFS IP: 172.20.10.10</li><li>Master IP: 172.20.10.11</li><li>Worker IP: 172.20.10.12</li><li>DNS: 8.8.8.8, 8.8.4.4 (Google’s DNS)</li></ul><h3 id=system-updateupgrade>System Update/Upgrade</h3><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo apt update <span style=color:#f92672>&amp;&amp;</span> sudo apt upgrade
sudo apt dist-upgrade <span style=color:#f92672>&amp;&amp;</span> sudo apt autoremove
sudo reboot
</code></pre></div><h3 id=time-sync>Time Sync</h3><p>Perform the following steps on all the machines:</p><ol><li><p>Set your timezone. Using <code>America/Los_Angeles</code> as an example:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>timedatectl list-timezones
sudo timedatectl set-timezone America/Los_Angeles
</code></pre></div></li><li><p>Confirm that the default timekeeping service is on (NTP service).</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>timedatectl
</code></pre></div></li><li><p>Install <code>chrony</code>.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo apt install chrony
</code></pre></div></li><li><p>Edit the configuration.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo nano /etc/chrony/chrony.conf
</code></pre></div><p>Add the following pools as the clock sources:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>pool time.google.com     iburst minpoll <span style=color:#ae81ff>1</span> maxpoll <span style=color:#ae81ff>2</span> maxsources <span style=color:#ae81ff>3</span>
pool us.pool.ntp.org     iburst minpoll <span style=color:#ae81ff>1</span> maxpoll <span style=color:#ae81ff>2</span> maxsources <span style=color:#ae81ff>3</span>
pool ntp.ubuntu.com      iburst minpoll <span style=color:#ae81ff>1</span> maxpoll <span style=color:#ae81ff>2</span> maxsources <span style=color:#ae81ff>3</span>
</code></pre></div><p>Update the two settings:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>maxupdateskew 5.0 <span style=color:#75715e># The threshold for determining whether an estimate is too unreliable to be used.</span>
makestep 0.1 -1  <span style=color:#75715e># This would step the system clock if the adjustment is larger than 0.1 seconds.</span>
</code></pre></div></li><li><p>Restart the service.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo systemctl restart chronyd
</code></pre></div></li><li><p>To see the source of synchronization data.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>chronyc sources
</code></pre></div><p>To view the current status of chrony.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>chronyc tracking
</code></pre></div></li></ol><h3 id=configure-firewall>Configure Firewall</h3><ol><li><p>Set up default rules.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo ufw default deny incoming
sudo ufw default allow outgoing
</code></pre></div></li><li><p>(<strong>Optional</strong>) We suggest changing the ssh port from <code>22</code> to another port for security. You can open the <code>sshd_config</code> config file and change the port setting:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo nano /etc/ssh/sshd_config
</code></pre></div><p>Change <code>Port 22</code> to your designated port and then restart the ssh service.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo service sshd restart
</code></pre></div><p>No matter which port is used, remember to allow incoming traffic to your ssh port over TCP:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo ufw allow &lt;ssh-port&gt;/tcp
</code></pre></div></li><li><p>On the NFS server, add the rule for NFS service:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo ufw allow 2049/tcp
</code></pre></div></li><li><p>On the master and worker machines, add the rules for MicroK8s services:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo ufw allow 16443/tcp
sudo ufw allow 10250/tcp
sudo ufw allow 10255/tcp
sudo ufw allow 25000/tcp
sudo ufw allow 12379/tcp
sudo ufw allow 10257/tcp
sudo ufw allow 10259/tcp
sudo ufw allow 19001/tcp
</code></pre></div></li><li><p>On the master and worker machines, add the rules for beacon node:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo ufw allow 12000/udp
sudo ufw allow 13000/tcp
</code></pre></div></li><li><p>Lastly, enable the firewall on each machine.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo ufw <span style=color:#111>enable</span>
sudo ufw status numbered
</code></pre></div></li></ol><h3 id=install-microk8s>Install MicroK8s</h3><p>To install MicroK8s, you can refer to <a href=https://microk8s.io/docs>MicroK8s’ official installation guide</a> or follow the instructions below on both of the master and worker machines.</p><ol><li><p>Install and run MicroK8s.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo snap install microk8s --classic --channel<span style=color:#f92672>=</span>1.20/stable
</code></pre></div></li><li><p>To grant your non-root user the admin privilege to execute MicroK8s commands, add the user to the MicroK8s group and change the owner of <code>~/.kube</code> directory.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo usermod -a -G microk8s <span style=color:#111>$USER</span>
sudo chown -f -R <span style=color:#111>$USER</span> ~/.kube

su - <span style=color:#111>$USER</span> <span style=color:#75715e># Re-enter the session for the update to take place.</span>
</code></pre></div></li><li><p>Wait for MicroK8s to be ready.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s status --wait-ready
</code></pre></div></li><li><p>Double check the node is ready.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s kubectl get node
</code></pre></div><p>You should only see one node in the result.</p></li></ol><h3 id=set-up-a-cluster>Set up a Cluster</h3><p>Please finish the previous section and make sure MicroK8s is running on both master and worker machines before proceeding.</p><p>On the master:</p><ol><li><p>Enable DNS and Helm3.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s <span style=color:#111>enable</span> dns helm3
</code></pre></div></li><li><p>Use the add-node command to generate a connection string for the worker node to join the cluster.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s add-node
</code></pre></div><p>You should see output like this:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>Join node with:
microk8s join 172.31.20.243:25000/DDOkUupkmaBezNnMheTBqFYHLWINGDbf

If the node you are adding is not reachable through the default
interface you can use one of the following:

microk8s join 172.31.20.243:25000/DDOkUupkmaBezNnMheTBqFYHLWINGDbf
microk8s join 10.1.84.0:25000/DDOkUupkmaBezNnMheTBqFYHLWINGDbf
</code></pre></div></li></ol><p>On the worker:</p><ol><li><p>Copy the join command and join the cluster. For example,</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s join 172.31.20.243:25000/DDOkUupkmaBezNnMheTBqFYHLWINGDbf
</code></pre></div></li><li><p>After the joining is done, check whether the worker node is in the cluster.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s kubectl get node
</code></pre></div><p>You should see both master and worker nodes in the result.</p></li></ol><h3 id=install-and-configure-nfs>Install and Configure NFS</h3><p>You can refer to the <a href=https://ubuntu.com/server/docs/service-nfs>guide for NFS installation and configuration on Ubuntu</a> or follow the instructions below.</p><p>On the machine you plan to run NFS:</p><ol><li><p>Install and start the NFS server.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo apt install nfs-kernel-server
sudo systemctl start nfs-kernel-server.service
</code></pre></div></li><li><p>Create directories for the beacon node, validators, and wallets.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo mkdir -p /data/prysm/beacon

sudo mkdir -p /data/prysm/validator-1 /data/prysm/wallet-1
sudo mkdir -p /data/prysm/validator-2 /data/prysm/wallet-2
</code></pre></div><p>Please note that each wallet can only be used by a single validator client. You can import multiple validator keys into the same wallet and use <strong>one validator client</strong> to attest/propose blocks for multiple validators.</p><p><em>To avoid slashing, do not use multiple validator clients with the same wallet or have the same key imported into different wallets used by different validator clients.</em></p></li><li><p>Configure and export NFS storage.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo nano /etc/exports
</code></pre></div><p>Add the following and save the file:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>/data *<span style=color:#f92672>(</span>rw,sync,no_subtree_check<span style=color:#f92672>)</span>
</code></pre></div><p>Option descriptions:</p><ul><li><strong>*</strong>: hostname format.</li><li><strong>rw</strong>: read/write permission.</li><li><strong>sync</strong>: changes are guaranteed to be committed to stable storage before replying to requests.</li><li><strong>no_subtree_check</strong>: disables subtree checking, which has mild security implications, but can improve reliability in some circumstances. From release 1.1.0 of nfs-utils onwards, the default is no_subtree_check as subtree_checking tends to cause more problems than it is worth.</li></ul><p>Please see the <a href=https://man7.org/linux/man-pages/man5/exports.5.html>NFS server export table manual</a> for more details.</p><p>Export the config</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo exportfs -a
</code></pre></div></li></ol><p>On your master and worker nodes, enable NFS support by installing nfs-common:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo apt install nfs-common
</code></pre></div><h3 id=prepare-validator-wallets>Prepare Validator Wallets</h3><p>Please refer to Prysm’s <a href=https://docs.prylabs.network/docs/mainnet/joining-eth2/#step-4-import-your-validator-accounts-into-prysm>official documentation</a>.</p><p>Let’s get back to the NFS server. We need to configure the wallet directories that we created in the previous section. Before proceeding, please have your validator keys placed on your NFS machine. To create a wallet and import your validator keys for Prysm validator clients, we use Prysm’s startup script.</p><ol><li><p>Please follow Prysm’s <a href=https://docs.prylabs.network/docs/install/install-with-script/#downloading-the-prysm-startup-script>documentation</a> to download Prysm startup script.</p></li><li><p>Execute the startup script with <code>--keys-dir=&lt;path/to/validator-keys></code> (Remember to replace it with the directory you place the keys). We use <code>$HOME/eth2.0-deposit-cli/validator_keys</code> as the example path to the validator keys.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo ./prysm.sh validator accounts import --keys-dir<span style=color:#f92672>=</span><span style=color:#111>$HOME</span>/eth2.0-deposit-cli/validator_keys
</code></pre></div></li><li><p>When prompted, enter your wallet directory. For example, <code>/data/prysm/wallet-1</code></p></li><li><p>Create the wallet password (<strong>remember to back it up somewhere safe!</strong>)</p></li><li><p>Enter the password you used to create the validator keys with the <a href=https://github.com/ethereum/eth2.0-deposit-cli>eth2.0-deposit-cli</a>. If you enter it correctly, the accounts will be imported into the new wallet.</p></li></ol><h3 id=change-the-owner-of-the-data-folder>Change the owner of the data folder</h3><p>On the NFS machine, let’s change the directory owners so later these directories can be mounted by Kubernetes as the storage volumes for the pods running the beacon node and the validator.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>sudo chown -R 1001:2000 /data <span style=color:#75715e># you can pick other user ID and group ID</span>
</code></pre></div><h3 id=prepare-the-helm-chart>Prepare the Helm Chart</h3><p>We understand it is not trivial to learn Kubernetes and create manifests or Helm charts for staking from scratch, so we’ve already done this for you to help you bootstrap! We uploaded all the manifests in our Github repository <a href=https://github.com/eth2xk8s/eth2xk8s>eth2xk8s</a>.</p><p>We use Helm to manage packages and releases in this guide. You can also use Kubernetes manifests directly. Please see <a href=https://github.com/eth2xk8s/eth2xk8s/blob/master/host-path/README.md>Testing with manifests and hostPath</a> and <a href=https://github.com/eth2xk8s/eth2xk8s/blob/master/nfs/README.md>Testing with manifests and NFS</a> for details.</p><ol><li><p>Clone this repo.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>git clone https://github.com/eth2xk8s/eth2xk8s.git
</code></pre></div></li><li><p>Change values in <a href=https://github.com/eth2xk8s/eth2xk8s/blob/master/eth2prysm/values.yaml>./eth2prysm/values.yaml</a>.</p><p>We recommend checking each field in <code>values.yaml</code> to determine the desired configuration. Fields that need to be changed or verified before installing the chart are the following ones:</p><ul><li><strong>nfs.serverIp</strong>: NFS server IP address.</li><li><strong>nfs.user</strong>: The user ID is used to run all processes in the container that accesses the NFS.</li><li><strong>nfs.group</strong>: The group ID is used to grant limited file access to the processes in the container.</li><li><strong>image.version</strong>: Prysm client version.</li><li><strong>beacon.dataVolumePath</strong>: The path to the data directory on the NFS for the beacon node.</li><li><strong>beacon.web3Provider</strong> and <strong>beacon.fallbackWeb3Providers</strong>: Ethereum 1 node endpoints.</li><li><strong>validators.validator1.dataVolumePath</strong>: The path to the data directory on the NFS for the validator.</li><li><strong>validators.validator1.walletVolumePath</strong>: The path to the data directory on the NFS for the wallet.</li><li><strong>validators.validator1.walletPassword</strong>: The wallet password.</li></ul></li></ol><h3 id=install-prysm-via-helm-chart>Install Prysm via Helm Chart</h3><p>Kubernetes has the concept of <a href=https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/>namespaces</a> to define scopes for names and isolate accesses between resources. We use <code>prysm</code> as the namespace for the Prysm client.</p><p>Helm uses <a href=https://helm.sh/docs/glossary/#release>releases</a> to track each of the chart installations. In this guide, we specify our release name as “eth2xk8s”, you can change it to anything you prefer.</p><p>On your master:</p><ol><li><p>Create the namespace.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s kubectl create namespace prysm
</code></pre></div></li><li><p>Install the Prysm client.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s helm3 install eth2xk8s ./eth2prysm -nprysm
</code></pre></div></li><li><p>Check the configurations used.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s helm3 get manifest eth2xk8s -nprysm
</code></pre></div></li></ol><h3 id=check-client-status>Check Client Status</h3><ol><li><p>Check the deployment status.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s kubectl get pod -nprysm -w
</code></pre></div><p>This command will watch for changes. You can monitor it until the beacon node and validators are all in RUNNING status.</p></li><li><p>Check the log of the beacon node.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s kubectl logs -f -nprysm -lapp<span style=color:#f92672>=</span>beacon
</code></pre></div></li><li><p>Check the log of the first validator.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s kubectl logs -f -nprysm -lapp<span style=color:#f92672>=</span>validator1
</code></pre></div><p>To check other validators, change -lapp to other validators’ names specified in <code>values.yaml</code>, <em>e.g.</em> for checking the second validator.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s kubectl logs -f -nprysm -lapp<span style=color:#f92672>=</span>validator2
</code></pre></div></li></ol><h3 id=upgrade-the-prysm-version-with-helm-chart>Upgrade the Prysm Version with Helm Chart</h3><p>Ethereum 2.0 client teams work hard to push new versions frequently. Ideally, we should try to keep up with the new releases to get the up-to-date patches and features! We suggest using Helm for upgrading to leverage its releases and lifecycle management:</p><ol><li><p>Check <a href=https://github.com/prysmaticlabs/prysm/releases>Prysm Github release page</a> to get the latest release version.</p></li><li><p>Modify the <code>image.version</code> in <code>values.yaml</code> to the latest version, <em>e.g.</em> v1.3.4.</p></li><li><p>Save <code>values.yaml</code> and upgrade the client with the Helm upgrade command.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s helm3 upgrade eth2xk8s ./eth2prysm -nprysm
</code></pre></div></li><li><p>Check the configurations to see if it picks up the new version correctly.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s helm3 get manifest eth2xk8s -nprysm
</code></pre></div></li><li><p>Refer to <a href=#check-client-status>Check Client Status</a> section to verify the client is running without issues.</p></li></ol><h3 id=roll-back-the-release-with-helm>Roll Back the Release with Helm</h3><p>Rolling back with Helm is usually as straightforward as upgrading when there’s no database schema changes involved. You can follow the steps below:</p><ol><li><p>Check Helm release history and find a “good” release. Note the target revision number.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s helm3 <span style=color:#111>history</span> eth2xk8s -nprysm
</code></pre></div></li><li><p>If we want to roll back to revision 4,</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s helm3 rollback eth2xk8s <span style=color:#ae81ff>4</span> -nprysm
</code></pre></div></li><li><p>Check the configurations used and refer to the <a href=#check-client-status>Check Client Status</a> section to verify the client is running without issues.</p></li></ol><p>If the rollback involves schema changes, please refer to <a href=#roll-back-the-release-with-helm-schema-changes>Appendix: Roll Back the Release with Helm (Schema Changes)</a> for details.</p><p>Sometimes, it’s not possible to downgrade to previous versions like described <a href=https://docs.prylabs.network/docs/prysm-usage/staying-up-to-date/#downgrading-between-major-version-bumps>here</a>. Please refer to the client team’s documentations for details before you downgrade the client.</p><h2 id=conclusion>Conclusion</h2><p>Thank you for reading to the end of this guide! We hope this guide paves the way for staking with Kubernetes. We will continue to contribute more guides about staking with Kubernetes to the community. We are currently developing the Helm Chart and guides for other Ethereum 2.0 clients. Stay tuned!</p><h2 id=feedback>Feedback</h2><p>We would love to hear from you! Let us know what you think.</p><ul><li>If you have any suggestions or questions regarding this guide, feel free to open issues or pull requests in our <a href=https://github.com/lumostone/lumostone.github.io>website</a> repository.</li><li>If you would like to contribute to the Helm Chart, open issues or pull requests in <a href=https://github.com/eth2xk8s/eth2xk8s>eth2xk8s</a> repository.</li></ul><h2 id=appendix>Appendix</h2><h3 id=check-cpu--memory-usage>Check CPU / Memory Usage</h3><p>You can use <a href=https://github.com/kubernetes-sigs/metrics-server>metrics server</a> to check the CPU/Memory usage of each pod.</p><ol><li><p>Install metrics server on the cluster:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s kubectl apply -f https://github.com/kubernetes-sigs/metrics-server/releases/latest/download/components.yaml
</code></pre></div></li><li><p>Run the <code>kubectl top</code> command, for example:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s kubectl top pod -l <span style=color:#111>app</span><span style=color:#f92672>=</span>beacon
kubectl top pod -l <span style=color:#111>app</span><span style=color:#f92672>=</span>validator
</code></pre></div></li></ol><h3 id=uninstall-helm-chart>Uninstall Helm Chart</h3><p>If you want to stop and uninstall the Prysm client, you can uninstall the Helm chart with the following command:</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s helm3 uninstall eth2xk8s -nprysm
</code></pre></div><h3 id=roll-back-the-release-with-helm-schema-changes>Roll Back the Release with Helm (Schema Changes)</h3><p>Take <a href=https://github.com/prysmaticlabs/prysm/releases/tag/v1.3.0>Prysm v1.3.0 release for example</a> as an example. If you decide to roll back to v1.2.x after upgrading to v1.3.0, you’ll need to run a script first to reverse the database migration. If we use instructions in <a href=#roll-back-the-release-with-helm>Roll Back the Release with Helm</a> directly, the pods will restart right after the version is changed by Helm and the client might not run due to the unmatched schema.</p><p>Hence, we can take advantage of Kubernetes to help us temporarily scale down the pods and then to run the reverse migration script before rolling back.</p><ol><li><p>Before rolling back the release, scale down the target deployment, <em>e.g</em>. scale down beacon node</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s kubectl scale deployments/beacon -nprysm --replicas<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
</code></pre></div><p>or scale down validator1 if the schema changes only affect validators</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s kubectl scale deployments/validator1 -nprysm --replicas<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>
</code></pre></div></li><li><p>Confirm that the pod(s) are terminated.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s kubectl get pod -nprysm -w
</code></pre></div></li><li><p>Run the reverse migration script.</p></li><li><p>Roll back the release to revision 4</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s helm3 rollback eth2xk8s <span style=color:#ae81ff>4</span> -nprysm
</code></pre></div></li><li><p>Scale up the deployment.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s kubectl scale deployments/beacon -nprysm --replicas<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
microk8s kubectl scale deployments/validator1 -nprysm --replicas<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>
</code></pre></div></li><li><p>Confirm that the pod(s) are running.</p><div class=highlight><pre style=color:#272822;background-color:#fafafa;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash>microk8s kubectl get pod -nprysm -w
</code></pre></div></li></ol></div><div class=post_footer></div></div><div class=doc_comments></div></div></div></div><div class=article-toc style=display:none><h3>Contents</h3><nav id=TableOfContents><ul><li><a href=#why-stake-with-kubernetes>Why Stake with Kubernetes?</a></li><li><a href=#acknowledgement>Acknowledgement</a></li><li><a href=#technologies>Technologies</a></li><li><a href=#goal>Goal</a></li><li><a href=#non-goal>Non-Goal</a></li><li><a href=#disclaimer>Disclaimer</a></li><li><a href=#system-requirements>System Requirements</a></li><li><a href=#prerequisites>Prerequisites</a></li><li><a href=#network-requirements>Network Requirements</a></li><li><a href=#walkthrough>Walkthrough</a><ul><li><a href=#overview>Overview</a></li><li><a href=#system-updateupgrade>System Update/Upgrade</a></li><li><a href=#time-sync>Time Sync</a></li><li><a href=#configure-firewall>Configure Firewall</a></li><li><a href=#install-microk8s>Install MicroK8s</a></li><li><a href=#set-up-a-cluster>Set up a Cluster</a></li><li><a href=#install-and-configure-nfs>Install and Configure NFS</a></li><li><a href=#prepare-validator-wallets>Prepare Validator Wallets</a></li><li><a href=#change-the-owner-of-the-data-folder>Change the owner of the data folder</a></li><li><a href=#prepare-the-helm-chart>Prepare the Helm Chart</a></li><li><a href=#install-prysm-via-helm-chart>Install Prysm via Helm Chart</a></li><li><a href=#check-client-status>Check Client Status</a></li><li><a href=#upgrade-the-prysm-version-with-helm-chart>Upgrade the Prysm Version with Helm Chart</a></li><li><a href=#roll-back-the-release-with-helm>Roll Back the Release with Helm</a></li></ul></li><li><a href=#conclusion>Conclusion</a></li><li><a href=#feedback>Feedback</a></li><li><a href=#appendix>Appendix</a><ul><li><a href=#check-cpu--memory-usage>Check CPU / Memory Usage</a></li><li><a href=#uninstall-helm-chart>Uninstall Helm Chart</a></li><li><a href=#roll-back-the-release-with-helm-schema-changes>Roll Back the Release with Helm (Schema Changes)</a></li></ul></li></ul></nav></div><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.1.1/jquery.slim.min.js integrity="sha256-/SIrNqv8h6QGKDuNoLGA4iret+kyesCkHGzVUUV0shc=" crossorigin=anonymous></script><script>(function(){var a=$('#TableOfContents'),b;if(a.length>0){b=$(window);function c(){var e=b.scrollTop(),f=$('.post_content h1, .post_content h2, .post_content h3, .post_content h4, .post_content h5, .post_content h6'),c="",d;if(f.each(function(b,a){a=$(a),a.offset().top-10<=e&&(c=a.attr('id'))}),d=a.find('a.active'),d.length==1&&d.eq(0).attr('href')=='#'+c)return!0;d.each(function(b,a){$(a).removeClass('active').siblings('ul').hide()}),a.find('a[href="#'+c+'"]').parentsUntil('#TableOfContents').each(function(b,a){$(a).children('a').addClass('active').siblings('ul').show()})}b.on('scroll',c),$(document).ready(function(){a.find('a').parent('li').find('ul').hide(),c(),document.getElementsByClassName('article-toc')[0].style.display=''})}})()</script><a id=back_to_top href=# class=back_to_top><i class=ri-arrow-up-s-line></i></a><footer class=footer><div class=powered_by><a href=https://github.com/lumostone/lumostone.github.io>Source code hosted on Github.</a></div><div class=footer_slogan><span></span></div></footer><script src=https://lumostone.com/js/jquery-3.5.1.min.js></script><link href=https://lumostone.com/css/fancybox.min.css rel=stylesheet><script src=https://lumostone.com/js/fancybox.min.js></script><script src=https://lumostone.com/js/zozo.js></script><script async src="https://www.googletagmanager.com/gtag/js?id=G-T9LBVECNMJ"></script><script>var doNotTrack=!1;if(!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag('js',new Date),gtag('config','G-T9LBVECNMJ',{anonymize_ip:!1})}</script></body></html>